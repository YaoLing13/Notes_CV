### 2.2 损失函数
![](imgs/20191201-162929.png)

### 2.3 梯度下降
![](imgs/20191201-164545.png)
![](imgs/20191201-164718.png)

线性回归的梯度下降
![](imgs/20191201-165838.png) 

***
***

## 4 多元线性
#### 4.1 feature scaling（归一化）
数据的归一化处理的重要性
$$ x_i \leftarrow \frac{x_i - u_i}{s_i} $$
![](imgs/20191201-173324.png)

#### 4.2 learning rate choose
![](imgs/20191201-174353.png)
![](imgs/20191201-174509.png)

#### 4.3 feature choose

#### 4.4 normal equration
![](imgs/20191201-181642.png)
$$ X \theta = y $$
$$ X^{T} X \theta = y $$
$$ \theta = [ X^{T} X ] ^{-1} X^{T} y $$